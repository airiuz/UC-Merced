{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45579bb5",
   "metadata": {},
   "source": [
    "### Uc_Merged dataseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e184c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'uc_merced'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f8eef",
   "metadata": {},
   "source": [
    "#### Tensorflow datasetlaridan uc_merged datasetini yuklab olamiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7263fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\n",
    "    data_name,\n",
    "    split=['train'],\n",
    "    data_dir=\"dataset\\\\\",\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    batch_size=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c059814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='uc_merced',\n",
       "    full_name='uc_merced/2.0.0',\n",
       "    description=\"\"\"\n",
       "    UC Merced is a 21 class land use remote sensing image dataset, with 100 images\n",
       "    per class. The images were manually extracted from large images from the USGS\n",
       "    National Map Urban Area Imagery collection for various urban areas around the\n",
       "    country. The pixel resolution of this public domain imagery is 0.3 m.\n",
       "    \n",
       "    While most images are 256x256 pixels, there are 44 images with different shape.\n",
       "    \"\"\",\n",
       "    homepage='http://weegee.vision.ucmerced.edu/datasets/landuse.html',\n",
       "    data_path='dataset\\\\uc_merced\\\\2.0.0',\n",
       "    file_format=tfrecord,\n",
       "    download_size=317.07 MiB,\n",
       "    dataset_size=238.63 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'filename': Text(shape=(), dtype=string),\n",
       "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=21),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'train': <SplitInfo num_examples=2100, num_shards=2>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{Nilsback08,\n",
       "       author = \"Yang, Yi and Newsam, Shawn\",\n",
       "       title = \"Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification\",\n",
       "       booktitle = \"ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM GIS)\",\n",
       "       year = \"2010\",\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630cf43",
   "metadata": {},
   "source": [
    "#### Image va labellarni preprocessingdan o'tkazamiz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7871de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels = tfds.as_numpy(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocessing(images,label):\n",
    "    images_ = []\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        image = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image,(224,224),interpolation = cv2.INTER_AREA)\n",
    "        image = image/127.5-1.0\n",
    "        image = image.astype('float16')\n",
    "        \n",
    "        images_.append(image)\n",
    "        \n",
    "    label = label.astype('uint16')\n",
    "    images = np.array(images_)\n",
    "    return images,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f2295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels = preprocessing(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e1becb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558b27a",
   "metadata": {},
   "source": [
    "#### Datasetni train va test datalarga ajratamiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d3bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((images[:2000], labels[:2000])).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((images[2000:], labels[2000:])).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461e8b5",
   "metadata": {},
   "source": [
    "### Model o'qitish uchun VGG-19 arxitekturasidan foydalanamiz\n",
    "#### 1. VGG-19 transfer learning yordamida o'qitish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d21f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = tf.keras.applications.VGG19(input_shape=(224,224,3), include_top=False) # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ba763",
   "metadata": {},
   "source": [
    "#####  weight va biaslarni Non-trainable holatga o'tkazamiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc050e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4ae44",
   "metadata": {},
   "source": [
    "##### Arxitekturaning Convolutional qismi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572ed49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d06653",
   "metadata": {},
   "source": [
    "##### Modelni ANN qismini datasetimizga moslab qurib olamiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ce736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vgg19,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(2048,activation = tf.keras.activations.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1024,activation = tf.keras.activations.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512,activation = tf.keras.activations.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(256,activation = tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(21,activation = tf.keras.activations.softmax),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec80c91",
   "metadata": {},
   "source": [
    "##### Fully-connected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e64fa9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              51382272  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             8192      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 21)                5397      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,180,693\n",
      "Trainable params: 54,149,141\n",
      "Non-trainable params: 20,031,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22455e",
   "metadata": {},
   "source": [
    "##### callbacks parametrlarini sozlab olamiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df70b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    patience=3)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'models\\VGG-19\\model_with_transfer.h5', \n",
    "     monitor='val_sparse_categorical_accuracy', mode='max', \n",
    "     save_best_only=True, \n",
    "     verbose=1)\n",
    "\n",
    "log_directory = \"logs\"\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_directory,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq=\"epoch\",\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf15ad",
   "metadata": {},
   "source": [
    "##### Modelni compile va fit qilamiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "046fa036",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0229 - sparse_categorical_accuracy: 0.6815\n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.37000, saving model to models\\VGG-19\\model_with_transfer.h5\n",
      "63/63 [==============================] - 260s 4s/step - loss: 1.0229 - sparse_categorical_accuracy: 0.6815 - val_loss: 8.4592 - val_sparse_categorical_accuracy: 0.3700\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2314 - sparse_categorical_accuracy: 0.9170\n",
      "Epoch 2: val_sparse_categorical_accuracy improved from 0.37000 to 0.52000, saving model to models\\VGG-19\\model_with_transfer.h5\n",
      "63/63 [==============================] - 261s 4s/step - loss: 0.2314 - sparse_categorical_accuracy: 0.9170 - val_loss: 3.7798 - val_sparse_categorical_accuracy: 0.5200\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1142 - sparse_categorical_accuracy: 0.9685\n",
      "Epoch 3: val_sparse_categorical_accuracy improved from 0.52000 to 0.84000, saving model to models\\VGG-19\\model_with_transfer.h5\n",
      "63/63 [==============================] - 260s 4s/step - loss: 0.1142 - sparse_categorical_accuracy: 0.9685 - val_loss: 0.4665 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0585 - sparse_categorical_accuracy: 0.9830\n",
      "Epoch 4: val_sparse_categorical_accuracy improved from 0.84000 to 0.91000, saving model to models\\VGG-19\\model_with_transfer.h5\n",
      "63/63 [==============================] - 260s 4s/step - loss: 0.0585 - sparse_categorical_accuracy: 0.9830 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0257 - sparse_categorical_accuracy: 0.9920\n",
      "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.91000\n",
      "63/63 [==============================] - 259s 4s/step - loss: 0.0257 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.4658 - val_sparse_categorical_accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics = tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "history = model.fit(train_ds, epochs = 5, validation_data = test_ds, callbacks = [early_stopping, checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e543b",
   "metadata": {},
   "source": [
    "#### 2. VGG-19 arxitekturasini noldan o'zimiz quramiz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12f4afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.block1_1conv = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block1_2conv = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        \n",
    "        self.block2_1conv = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block2_2conv = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        \n",
    "        self.block3_1conv = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block3_2conv = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block3_3conv = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block3_4conv = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        \n",
    "        self.block4_1conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block4_2conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block4_3conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block4_4conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        \n",
    "        self.block5_1conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block5_2conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block5_3conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        self.block5_4conv = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.relu, padding=\"same\")\n",
    "        \n",
    "        self.pool = tf.keras.layers.MaxPooling2D((2, 2),strides=2)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.hidden_layer1 = tf.keras.layers.Dense(2048, activation=tf.keras.activations.relu)\n",
    "        self.hidden_layer2 = tf.keras.layers.Dense(1024, activation=tf.keras.activations.relu)\n",
    "        self.hidden_layer3 = tf.keras.layers.Dense(512, activation=tf.keras.activations.relu)\n",
    "        self.hidden_layer4 = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)        \n",
    "        self.output_layer = tf.keras.layers.Dense(21, activation=tf.keras.activations.softmax)\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        '''Convolutional Layer'''\n",
    "        # Block 1\n",
    "        conv1 = self.block1_1conv(x) # 224x224x64\n",
    "        conv2 = self.block1_2conv(conv1) # 224x224x64\n",
    "        pooling = self.pool(conv2) # 112x112x64\n",
    "\n",
    "        # Block 2\n",
    "        conv3 = self.block2_1conv(pooling) # 112x112x128\n",
    "        conv4 = self.block2_2conv(conv3) # 112x112x128\n",
    "        pooling = self.pool(conv4) # 56x56x128\n",
    "\n",
    "        # Block 3\n",
    "        conv5 = self.block3_1conv(pooling) # 56x56x256\n",
    "        conv6 = self.block3_2conv(conv5) # 56x56x256\n",
    "        conv7 = self.block3_3conv(conv6) # 56x56x256\n",
    "        conv8 = self.block3_4conv(conv7) # 56x56x256\n",
    "        pooling = self.pool(conv8) # 28x28x256\n",
    "        \n",
    "        # Block 4\n",
    "        conv9  = self.block4_1conv(pooling) # 28x28x512\n",
    "        conv10 = self.block4_2conv(conv9) # 28x28x512\n",
    "        conv11 = self.block4_3conv(conv10) # 28x28x512\n",
    "        conv12 = self.block4_4conv(conv11) # 28x28x512\n",
    "        pooling = self.pool(conv12) # 14x14x512\n",
    "        \n",
    "        # Block 5\n",
    "        conv13 = self.block5_1conv(pooling) # 14x14x512\n",
    "        conv14 = self.block5_2conv(conv13) # 14x14x512\n",
    "        conv15 = self.block5_3conv(conv14) # 14x14x512\n",
    "        conv16 = self.block5_4conv(conv15) # 14x14x512\n",
    "        pooling = self.pool(conv16) # 7x7x512\n",
    "        \n",
    "        '''Classification Layer'''\n",
    "        flatten = self.flatten(pooling)\n",
    "        \n",
    "        hidden1 = self.hidden_layer1(flatten)\n",
    "        hidden2 = self.hidden_layer2(hidden1)\n",
    "        hidden3 = self.hidden_layer3(hidden2)\n",
    "        hidden4 = self.hidden_layer4(hidden3)\n",
    "        \n",
    "        return self.output_layer(hidden4)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bd04a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa8d37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca76ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86fee5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd77f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 1-epoch:\n",
      "loss: 3.0516700744628906\tsparse_categorical_accuracy: 0.039000000804662704\n",
      "val_loss: 3.046691417694092\tval_sparse_categorical_accuracy: 0.03999999910593033\n",
      "** 2-epoch:\n",
      "loss: 3.0452542304992676\tsparse_categorical_accuracy: 0.03700000047683716\n",
      "val_loss: 3.046025276184082\tval_sparse_categorical_accuracy: 0.03999999910593033\n",
      "** 3-epoch:\n",
      "loss: 3.044947624206543\tsparse_categorical_accuracy: 0.03700000047683716\n",
      "val_loss: 3.0476186275482178\tval_sparse_categorical_accuracy: 0.029999999329447746\n",
      "** 4-epoch:\n",
      "loss: 3.0448215007781982\tsparse_categorical_accuracy: 0.04149999842047691\n",
      "val_loss: 3.0484046936035156\tval_sparse_categorical_accuracy: 0.029999999329447746\n",
      "** 5-epoch:\n",
      "loss: 3.044779062271118\tsparse_categorical_accuracy: 0.04100000113248825\n",
      "val_loss: 3.049091339111328\tval_sparse_categorical_accuracy: 0.029999999329447746\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    print(f\"** {epoch + 1}-epoch:\\nloss: {train_loss.result()}\\tsparse_categorical_accuracy: {train_accuracy.result()}\\nval_loss: {test_loss.result()}\\tval_sparse_categorical_accuracy: {test_accuracy.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69e4e49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  36928     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           multiple                  73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           multiple                  147584    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           multiple                  295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           multiple                  590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           multiple                  590080    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           multiple                  590080    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           multiple                  1180160   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           multiple                  2359808   \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          multiple                  2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          multiple                  2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          multiple                  2359808   \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          multiple                  2359808   \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          multiple                  2359808   \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          multiple                  2359808   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  51382272  \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  2098176   \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  524800    \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  131328    \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  5397      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,166,357\n",
      "Trainable params: 74,166,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceaed1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/VGG-19/model_without_transfer\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/VGG-19/model_without_transfer\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('models/VGG-19/model_without_transfer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
